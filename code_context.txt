// Root Files
// File: README.md
# Web-scale Data Management Project Template

Basic project structure with Python's Flask and Redis. 
**You are free to use any web framework in any language and any database you like for this project.**

### Project structure

* `env`
    Folder containing the Redis env variables for the docker-compose deployment
    
* `helm-config` 
   Helm chart values for Redis and ingress-nginx
        
* `k8s`
    Folder containing the kubernetes deployments, apps and services for the ingress, order, payment and stock services.
    
* `order`
    Folder containing the order application logic and dockerfile. 
    
* `payment`
    Folder containing the payment application logic and dockerfile. 

* `stock`
    Folder containing the stock application logic and dockerfile. 

* `test`
    Folder containing some basic correctness tests for the entire system. (Feel free to enhance them)

### Deployment types:

#### docker-compose (local development)

After coding the REST endpoint logic run `docker-compose up --build` in the base folder to test if your logic is correct
(you can use the provided tests in the `\test` folder and change them as you wish). 

***Requirements:*** You need to have docker and docker-compose installed on your machine. 

K8s is also possible, but we do not require it as part of your submission. 

#### minikube (local k8s cluster)

This setup is for local k8s testing to see if your k8s config works before deploying to the cloud. 
First deploy your database using helm by running the `deploy-charts-minicube.sh` file (in this example the DB is Redis 
but you can find any database you want in https://artifacthub.io/ and adapt the script). Then adapt the k8s configuration files in the
`\k8s` folder to mach your system and then run `kubectl apply -f .` in the k8s folder. 

***Requirements:*** You need to have minikube (with ingress enabled) and helm installed on your machine.

#### kubernetes cluster (managed k8s cluster in the cloud)

Similarly to the `minikube` deployment but run the `deploy-charts-cluster.sh` in the helm step to also install an ingress to the cluster. 

***Requirements:*** You need to have access to kubectl of a k8s cluster.

### OpenTelemetry

OpenTelemetry is an observability framework for distributed applications.
It provides three high-level components: Tracing, Metrics and Logging (although logging is not yet fully supported in Python).
Default configurations are provided for common libraries, such as Flask, Redis, etc.
These default configurations have been enabled in the stock, payment and order services and their DBs.
Use `docker-compose up --build` to start up the application.
This also starts the Aspire dashboard, which collects the telemetry data and visualizes it.
The dashboard is available on http://localhost:18888.
// File: code_context.txt

// File: deploy-charts-cluster.sh
#!/usr/bin/env bash

helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

helm repo update

helm install -f helm-config/redis-helm-values.yaml redis bitnami/redis
helm install -f helm-config/nginx-helm-values.yaml nginx ingress-nginx/ingress-nginx
// File: deploy-charts-minikube.sh
#!/usr/bin/env bash

helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

helm install -f helm-config/redis-helm-values.yaml redis bitnami/redis
// File: docker-compose.yml
version: "3"
services:

  gateway:
    image: nginx:1.25-bookworm
    volumes:
      - ./gateway_nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "8000:80"
    depends_on:
      - order-service
      - stock-service
      - payment-service

  order-service:
    build:
      context: .
      dockerfile: ./order/Dockerfile
    image: order:latest
    environment:
    - GATEWAY_URL=http://gateway:80
    - OTEL_EXPORTER_OTLP_ENDPOINT=http://host.docker.internal:4317
    - OTEL_TRACES_EXPORTER=otlp
    - OTEL_METRICS_EXPORTER=otlp
    - OTEL_LOGS_EXPORTER=otlp
    - OTEL_SERVICE_NAME=order-service
    - OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true
    - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    command: opentelemetry-instrument hypercorn app:app --bind 0.0.0.0:5000 --workers 1 --graceful-timeout 30 --log-level info
    env_file:
      - env/order_redis.env
    depends_on:
      - kafka
      - order-db
    restart: always
  order-db:
    image: redis:7.2-bookworm
    command: redis-server --requirepass redis --maxmemory 512mb

  stock-service:
    build:
      context: .
      dockerfile: ./stock/Dockerfile
    image: stock:latest
    environment:
      - GATEWAY_URL=http://gateway:80
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://host.docker.internal:4317
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=stock-service
      - OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true
    command: opentelemetry-instrument hypercorn app:app --bind 0.0.0.0:5000 --workers 1 --graceful-timeout 30 --log-level info
    env_file:
      - env/stock_redis.env
    depends_on:
      - kafka
      - stock-db
    restart: always

  stock-db:
    image: redis:7.2-bookworm
    command: redis-server --requirepass redis --maxmemory 512mb

  payment-service:
    build:
      context: .
      dockerfile: ./payment/Dockerfile
    image: user:latest
    environment:
      - GATEWAY_URL=http://gateway:80
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://host.docker.internal:4317
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=payment-service
      - OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true
    command: opentelemetry-instrument hypercorn app:app --bind 0.0.0.0:5000 --workers 1 --graceful-timeout 30 --log-level info
    env_file:
      - env/payment_redis.env
    depends_on:
      - kafka
      - payment-db

  payment-db:
    image: redis:7.2-bookworm
    command: redis-server --requirepass redis --maxmemory 512mb

  aspire-dashboard:
    image: mcr.microsoft.com/dotnet/aspire-dashboard:8.1.0
    ports:
      - "18888:18888" # Dashboard UI
      - "4317:18889"  # OTLP receiver
    environment:
      - DOTNET_DASHBOARD_UNSECURED_ALLOW_ANONYMOUS=true

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment :
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"




// File: gateway_nginx.conf
events { worker_connections 2048;}

http {
    upstream order-app {
        server order-service:5000;
    }
    upstream payment-app {
        server payment-service:5000;
    }
    upstream stock-app {
        server stock-service:5000;
    }
    server {
        listen 80;
        location /orders/ {
           proxy_pass   http://order-app/;
        }
        location /payment/ {
           proxy_pass   http://payment-app/;
        }
        location /stock/ {
           proxy_pass   http://stock-app/;
        }
        access_log  /var/log/nginx/server.access.log;
    }
    access_log  /var/log/nginx/access.log;
}

// File: get_context.sh
#!/bin/bash

# Usa la directory corrente come directory del progetto
project_dir=$(pwd)

# Usa un nome fisso per il file di output nella directory corrente
output_file="${project_dir}/code_context.txt"

# Controlla se il file di output esiste e rimuovilo se presente
if [ -f "$output_file" ]; then
  rm "$output_file"
fi

# Lista delle directory da includere (basata sulla struttura della repo)
directories=("env" "helm-config" "k8s" "order" "payment" "stock" "test")

# Lista dei tipi di file da ignorare
ignore_files=("*.ico" "*.png" "*.jpg" "*.jpeg" "*.gif" "*.svg" "*.pyc" "*.log")

# Funzione per calcolare il livello di indentazione
get_indent() {
  local path="$1"
  echo "$path" | awk -F'/' '{print length($0)-length(gensub("[^/]","","g",$0))}'
}

# Funzione ricorsiva per leggere i file e aggiungere il contenuto al file di output
read_files() {
  local current_indent="$2"  # Indentazione attuale per riflettere la struttura

  for entry in "$1"/*; do
    if [ -d "$entry" ]; then
      # Se è una directory, stampiamo il nome e continuiamo in modo ricorsivo
      relative_path=${entry#"$project_dir/"}
      indent=$(printf '  %.0s' $(seq 1 $current_indent))  # Indentazione
      echo "${indent}// Directory: $relative_path" >> "$output_file"
      read_files "$entry" $((current_indent + 1))  # Aumenta indentazione
    elif [ -f "$entry" ]; then
      # Controlla se il file deve essere ignorato
      should_ignore=false
      for ignore_pattern in "${ignore_files[@]}"; do
        if [[ "$entry" == $ignore_pattern ]]; then
          should_ignore=true
          break
        fi
      done

      # Se il file non è ignorato, aggiungilo al file di output
      if ! $should_ignore; then
        relative_path=${entry#"$project_dir/"}
        indent=$(printf '  %.0s' $(seq 1 $current_indent))  # Indentazione
        echo "${indent}// File: $relative_path" >> "$output_file"
        cat "$entry" >> "$output_file"
        echo "" >> "$output_file"
      fi
    fi
  done
}

# Includere anche i file nella root della repo
echo "// Root Files" >> "$output_file"
for file in "$project_dir"/*; do
  if [ -f "$file" ]; then
    should_ignore=false
    for ignore_pattern in "${ignore_files[@]}"; do
      if [[ "$file" == $ignore_pattern ]]; then
        should_ignore=true
        break
      fi
    done

    if ! $should_ignore; then
      relative_path=${file#"$project_dir/"}
      echo "// File: $relative_path" >> "$output_file"
      cat "$file" >> "$output_file"
      echo "" >> "$output_file"
    fi
  fi
done

# Chiama la funzione ricorsiva per ciascuna directory specificata
for dir in "${directories[@]}"; do
  if [ -d "${project_dir}/${dir}" ]; then
    echo "// Directory: $dir" >> "$output_file"
    read_files "${project_dir}/${dir}" 1  # Inizia con indentazione di 1
  fi
done

echo "Estrazione completata! Il contenuto è stato salvato in $output_file."

// File: requirements.txt
requests==2.31.0
Flask==3.0.2
redis==5.0.3
gunicorn==21.2.0
msgspec~=0.18.6
opentelemetry-distro
opentelemetry-exporter-otlp-proto-grpc
// Directory: env
  // File: env/order_redis.env
REDIS_HOST=order-db
REDIS_PORT=6379
REDIS_PASSWORD=redis
REDIS_DB=0
  // File: env/payment_redis.env
REDIS_HOST=payment-db
REDIS_PORT=6379
REDIS_PASSWORD=redis
REDIS_DB=0
  // File: env/stock_redis.env
REDIS_HOST=stock-db
REDIS_PORT=6379
REDIS_PASSWORD=redis
REDIS_DB=0
// Directory: helm-config
  // File: helm-config/nginx-helm-values.yaml
resources:
  limits:
    cpu: 500m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 1Gi
  // File: helm-config/redis-helm-values.yaml
auth:
  password: redis
master:
  persistence:
    size: 4Gi
  resources:
    requests:
      memory: 4Gi
      cpu: 2
replica:
  replicaCount: 1
  persistence:
    size: 4Gi
  resources:
    requests:
      memory: 4Gi
      cpu: 2
// Directory: k8s
  // File: k8s/ingress-service.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
 name: ingress-service
 annotations:
   kubernetes.io/ingress.class: nginx
   nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
 rules:
   - http:
      paths:
        - path: /orders/?(.*)
          pathType: Prefix
          backend:
            service:
              name: order-service
              port:
                number: 5000
        - path: /stock/?(.*)
          pathType: Prefix
          backend:
            service:
              name: stock-service
              port:
                number: 5000
        - path: /payment/?(.*)
          pathType: Prefix
          backend:
            service:
              name: user-service
              port:
                number: 5000
  // File: k8s/order-app.yaml
apiVersion: v1
kind: Service
metadata:
  name: order-service
spec:
  type: ClusterIP
  selector:
    component: order
  ports:
    - port: 5000
      name: http
      targetPort: 5000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: order
  template:
    metadata:
      labels:
        component: order
    spec:
      containers:
        - name: order
          image: order:latest
          resources:
            limits:
              memory: "1Gi"
              cpu: "1"
            requests:
              memory: "1Gi"
              cpu: "1"
          command: ["gunicorn"]
          args: ["-b", "0.0.0.0:5000", "app:app"]
          ports:
            - containerPort: 5000
          env:
            - name: USER_SERVICE_URL
              value: "user-service"
            - name: STOCK_SERVICE_URL
              value: "stock-service"
            - name: REDIS_HOST
              value: redis-master
            - name: REDIS_PORT
              value: '6379'
            - name: REDIS_PASSWORD
              value: "redis"
            - name: REDIS_DB
              value: "0"
  // File: k8s/stock-app.yaml
apiVersion: v1
kind: Service
metadata:
  name: stock-service
spec:
  type: ClusterIP
  selector:
    component: stock
  ports:
    - port: 5000
      name: http
      targetPort: 5000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stock-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: stock
  template:
    metadata:
      labels:
        component: stock
    spec:
      containers:
        - name: stock
          image: stock:latest
          resources:
            limits:
              memory: "1Gi"
              cpu: "1"
            requests:
              memory: "1Gi"
              cpu: "1"
          command: ["gunicorn"]
          args: ["-b", "0.0.0.0:5000", "app:app"]
          ports:
            - containerPort: 5000
          env:
            - name: REDIS_HOST
              value: redis-master
            - name: REDIS_PORT
              value: '6379'
            - name: REDIS_PASSWORD
              value: "redis"
            - name: REDIS_DB
              value: "0"

  // File: k8s/user-app.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
spec:
  type: ClusterIP
  selector:
    component: user
  ports:
    - port: 5000
      name: http
      targetPort: 5000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: user
  template:
    metadata:
      labels:
        component: user
    spec:
      containers:
        - name: user
          image: user:latest
          resources:
            limits:
              memory: "1Gi"
              cpu: "1"
            requests:
              memory: "1Gi"
              cpu: "1"
          command: ["gunicorn"]
          args: ["-b", "0.0.0.0:5000", "app:app"]
          ports:
            - containerPort: 5000
          env:
            - name: REDIS_HOST
              value: redis-master
            - name: REDIS_PORT
              value: '6379'
            - name: REDIS_PASSWORD
              value: "redis"
            - name: REDIS_DB
              value: "0"

// Directory: order
  // File: order/Dockerfile
FROM python:3.12-slim

WORKDIR /home/flask-app

COPY ./order .
COPY ./common ./common

RUN pip install -r requirements.txt
RUN opentelemetry-bootstrap -a install

EXPOSE 5000


  // File: order/__init__.py

  // File: order/app.py
import logging
import os
import atexit
import random
import uuid
from collections import defaultdict

import requests
import redis.asyncio as redis
from msgspec import msgpack, Struct
from quart import Quart, jsonify, abort, Response
from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
import json

from common.kafka.kakfaProducer import KafkaProducerSingleton



KAFKA_BOOTSTRAP_SERVERS = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")


logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)


from common.otlp_grcp_config import configure_telemetry


DB_ERROR_STR = "DB error"
REQ_ERROR_STR = "Requests error"

GATEWAY_URL = os.environ['GATEWAY_URL']

app = Quart("order-service")

db = redis.Redis(
    host=os.environ['REDIS_HOST'],
    port=int(os.environ['REDIS_PORT']),
    password=os.environ['REDIS_PASSWORD'],
    db=int(os.environ['REDIS_DB'])
)

configure_telemetry('order-service')

async def close_db_connection():
    await db.close()

class OrderValue(Struct):
    paid: bool
    items: list[tuple[str, int]]
    user_id: str
    total_cost: int


async def get_order_from_db(order_id: str) -> OrderValue | None:
    try:
        entry: bytes = await db.get(order_id)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    # deserialize data if it exists else return null
    entry: OrderValue | None = msgpack.decode(entry, type=OrderValue) if entry else None
    if entry is None:
        # if order does not exist in the database; abort
        abort(400, f"Order: {order_id} not found!")
    return entry


#    @app.post('/create/<user_id>')
#    async def create_order(user_id: str):
#        key = str(uuid.uuid4())
#        value = msgpack.encode(OrderValue(paid=False, items=[], user_id=user_id, total_cost=0))
#        try:
#            await db.set(key, value)
#        except redis.exceptions.RedisError:
#            return abort(400, DB_ERROR_STR)
#        return jsonify({'order_id': key})

@app.post('/create/<user_id>')
async def create_order(user_id: str):
    key = str(uuid.uuid4())
    order = OrderValue(paid=False, items=[], user_id=user_id, total_cost=0)
    encoded_order = msgpack.encode(order)
    try:
        await db.set(key, encoded_order)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    
    # Costruisci l'evento da inviare
    event = {
        "type": "OrderCreated",
        "order_id": key,
        "user_id": user_id,
        "paid": False,
        "items": [],
        "total_cost": 0
    }
    await KafkaProducerSingleton.send_event("orders", "order-created", event)
    
    return jsonify({'order_id': key})

@app.post('/batch_init/<n>/<n_items>/<n_users>/<item_price>')
async def batch_init_users(n: int, n_items: int, n_users: int, item_price: int):

    n = int(n)
    n_items = int(n_items)
    n_users = int(n_users)
    item_price = int(item_price)

    def generate_entry() -> OrderValue:
        user_id = random.randint(0, n_users - 1)
        item1_id = random.randint(0, n_items - 1)
        item2_id = random.randint(0, n_items - 1)
        value = OrderValue(paid=False,
                           items=[(f"{item1_id}", 1), (f"{item2_id}", 1)],
                           user_id=f"{user_id}",
                           total_cost=2*item_price)
        return value

    kv_pairs: dict[str, bytes] = {f"{i}": msgpack.encode(generate_entry())
                                  for i in range(n)}
    try:
        await db.mset(kv_pairs)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return jsonify({"msg": "Batch init for orders successful"})


@app.get('/find/<order_id>')
async def find_order(order_id: str):
    order_entry: OrderValue = await get_order_from_db(order_id)
    return jsonify(
        {
            "order_id": order_id,
            "paid": order_entry.paid,
            "items": order_entry.items,
            "user_id": order_entry.user_id,
            "total_cost": order_entry.total_cost
        }
    )


def send_post_request(url: str):
    try:
        response = requests.post(url)
    except requests.exceptions.RequestException:
        abort(400, REQ_ERROR_STR)
    else:
        return response


def send_get_request(url: str):
    try:
        response = requests.get(url)
    except requests.exceptions.RequestException:
        abort(400, REQ_ERROR_STR)
    else:
        return response


@app.post('/addItem/<order_id>/<item_id>/<quantity>')
async def add_item(order_id: str, item_id: str, quantity: int):
    order_entry: OrderValue = await get_order_from_db(order_id)
    item_reply = send_get_request(f"{GATEWAY_URL}/stock/find/{item_id}")
    if item_reply.status_code != 200:
        # Request failed because item does not exist
        abort(400, f"Item: {item_id} does not exist!")
    item_json: dict = item_reply.json()
    order_entry.items.append((item_id, int(quantity)))
    order_entry.total_cost += int(quantity) * item_json["price"]
    try:
        await db.set(order_id, msgpack.encode(order_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return Response(f"Item: {item_id} added to: {order_id} price updated to: {order_entry.total_cost}",
                    status=200)


def rollback_stock(removed_items: list[tuple[str, int]]):
    for item_id, quantity in removed_items:
        send_post_request(f"{GATEWAY_URL}/stock/add/{item_id}/{quantity}")


@app.post('/checkout/<order_id>')
async def checkout(order_id: str):
    app.logger.debug(f"Checking out {order_id}")
    order_entry: OrderValue = await get_order_from_db(order_id)
    # get the quantity per item
    items_quantities: dict[str, int] = defaultdict(int)
    for item_id, quantity in order_entry.items:
        items_quantities[item_id] += quantity
    # The removed items will contain the items that we already have successfully subtracted stock from
    # for rollback purposes.
    removed_items: list[tuple[str, int]] = []
    for item_id, quantity in items_quantities.items():
        stock_reply = send_post_request(f"{GATEWAY_URL}/stock/subtract/{item_id}/{quantity}")
        if stock_reply.status_code != 200:
            # If one item does not have enough stock we need to rollback
            rollback_stock(removed_items)
            abort(400, f'Out of stock on item_id: {item_id}')
        removed_items.append((item_id, quantity))
    user_reply = send_post_request(f"{GATEWAY_URL}/payment/pay/{order_entry.user_id}/{order_entry.total_cost}")
    if user_reply.status_code != 200:
        # If the user does not have enough credit we need to rollback all the item stock subtractions
        rollback_stock(removed_items)
        abort(400, "User out of credit")
    order_entry.paid = True
    try:
        await db.set(order_id, msgpack.encode(order_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    app.logger.debug("Checkout successful")
    return Response("Checkout successful", status=200)

@app.before_serving
async def startup():
    app.logger.info("Starting Order Service")
    await KafkaProducerSingleton.get_instance(KAFKA_BOOTSTRAP_SERVERS)
    startup_event = {
        "type": "AppStarted",
        "service": "order-service",
        "message": "Order Service is up and running!"
    }
    await KafkaProducerSingleton.send_event("app-events", "order-startup", startup_event)

@app.after_serving
async def shutdown():
    app.logger.info("Stopping Order Service")
    await KafkaProducerSingleton.close()
    await close_db_connection()
    

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)
    app.logger.setLevel(logging.INFO)
else:
    hypercorn_logger = logging.getLogger('hypercorn.error')
    app.logger.handlers = hypercorn_logger.handlers
    app.logger.setLevel(hypercorn_logger.level)

  // File: order/requirements.txt
Quart==0.20.0
redis==5.0.3
hypercorn==0.15.0 
msgspec==0.18.6
requests==2.31.0
opentelemetry-distro
opentelemetry-exporter-otlp-proto-grpc
aiokafka==0.9.0  
// Directory: payment
  // File: payment/Dockerfile
FROM python:3.12-slim

WORKDIR /home/flask-app

COPY ./payment .
COPY ./common ./common

RUN pip install -r requirements.txt
RUN opentelemetry-bootstrap -a install

EXPOSE 5000
  // File: payment/__init__.py

  // File: payment/app.py
import logging
import os
import atexit
import uuid

import redis.asyncio as redis

from msgspec import msgpack, Struct
from quart import Quart, jsonify, abort, Response

from common.otlp_grcp_config import configure_telemetry

DB_ERROR_STR = "DB error"

app = Quart("payment-service")

db = redis.Redis(
    host=os.environ['REDIS_HOST'],
    port=int(os.environ['REDIS_PORT']),
    password=os.environ['REDIS_PASSWORD'],
    db=int(os.environ['REDIS_DB'])
)

configure_telemetry('payment-service')


async def close_db_connection():
    await db.close()


class UserValue(Struct):
    credit: int


async def get_user_from_db(user_id: str) -> UserValue | None:
    try:
        # get serialized data
        entry: bytes = await db.get(user_id)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    # deserialize data if it exists else return null
    entry: UserValue | None = msgpack.decode(entry, type=UserValue) if entry else None
    if entry is None:
        # if user does not exist in the database; abort
        abort(400, f"User: {user_id} not found!")
    return entry


@app.post('/create_user')
async def create_user():
    key = str(uuid.uuid4())
    value = msgpack.encode(UserValue(credit=0))
    try:
        await db.set(key, value)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return jsonify({'user_id': key})


@app.post('/batch_init/<n>/<starting_money>')
async def batch_init_users(n: int, starting_money: int):
    n = int(n)
    starting_money = int(starting_money)
    kv_pairs: dict[str, bytes] = {f"{i}": msgpack.encode(UserValue(credit=starting_money))
                                  for i in range(n)}
    try:
        await db.mset(kv_pairs)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return jsonify({"msg": "Batch init for users successful"})


@app.get('/find_user/<user_id>')
async def find_user(user_id: str):
    user_entry: UserValue = await get_user_from_db(user_id)
    return jsonify(
        {
            "user_id": user_id,
            "credit": user_entry.credit
        }
    )


@app.post('/add_funds/<user_id>/<amount>')
async def add_credit(user_id: str, amount: int):
    user_entry: UserValue = await get_user_from_db(user_id)
    # update credit, serialize and update database
    user_entry.credit += int(amount)
    try:
        await db.set(user_id, msgpack.encode(user_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return Response(f"User: {user_id} credit updated to: {user_entry.credit}", status=200)


@app.post('/pay/<user_id>/<amount>')
async def remove_credit(user_id: str, amount: int):
    app.logger.debug(f"Removing {amount} credit from user: {user_id}")
    user_entry: UserValue = await get_user_from_db(user_id)
    # update credit, serialize and update database
    user_entry.credit -= int(amount)
    if user_entry.credit < 0:
        abort(400, f"User: {user_id} credit cannot get reduced below zero!")
    try:
        await db.set(user_id, msgpack.encode(user_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return Response(f"User: {user_id} credit updated to: {user_entry.credit}", status=200)

@app.before_serving
async def startup():
    app.logger.info("Starting Payment Service")

@app.after_serving
async def shutdown():
    app.logger.info("Stopping Payment Service")
    await close_db_connection()

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)
    app.logger.setLevel(logging.INFO)
else:
    hypercorn_logger = logging.getLogger('hypercorn.error')
    app.logger.handlers = hypercorn_logger.handlers
    app.logger.setLevel(hypercorn_logger.level)


  // File: payment/requirements.txt
Quart==0.20.0
redis==5.0.3
hypercorn==0.15.0 
msgspec==0.18.6
requests==2.31.0
aiokafka==0.9.0  
opentelemetry-distro
opentelemetry-exporter-otlp-proto-grpc
// Directory: stock
  // File: stock/Dockerfile
FROM python:3.12-slim

WORKDIR /home/flask-app

COPY ./stock .
COPY ./common ./common

RUN pip install -r requirements.txt
RUN opentelemetry-bootstrap -a install

EXPOSE 5000
  // File: stock/__init__.py

  // File: stock/app.py
import asyncio
import json
import logging
import os
import atexit
import uuid

from aiokafka import AIOKafkaConsumer
import redis.asyncio as redis 
from msgspec import msgpack, Struct
from quart import Quart, jsonify, abort, Response
from common.kafka.kafkaConsumer import KafkaConsumerSingleton
from opentelemetry import trace, metrics

from common.otlp_grcp_config import configure_telemetry




logging.basicConfig(
    level=logging.INFO,  # Forza a mostrare anche i log INFO
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler()  # Stampa su stdout
    ]
)


KAFKA_BOOTSTRAP_SERVERS = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
TOPICS = ["orders", "app-events"]


DB_ERROR_STR = "DB error"

app = Quart("stock-service")

db = redis.Redis(
    host=os.environ['REDIS_HOST'],
    port=int(os.environ['REDIS_PORT']),
    password=os.environ['REDIS_PASSWORD'],
    db=int(os.environ['REDIS_DB'])
)

configure_telemetry('stock-service')

async def close_db_connection():
    await db.close()


class StockValue(Struct):
    stock: int
    price: int


async def get_item_from_db(item_id: str) -> StockValue | None:
    # get serialized data
    try:
        entry: bytes = await db.get(item_id)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    # deserialize data if it exists else return null
    entry: StockValue | None = msgpack.decode(entry, type=StockValue) if entry else None
    if entry is None:
        # if item does not exist in the database; abort
        abort(400, f"Item: {item_id} not found!")
    return entry


@app.post('/item/create/<price>')
async def create_item(price: int):
    key = str(uuid.uuid4())
    app.logger.debug(f"Item: {key} created")
    value = msgpack.encode(StockValue(stock=0, price=int(price)))
    try:
        await db.set(key, value)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return jsonify({'item_id': key})


@app.post('/batch_init/<n>/<starting_stock>/<item_price>')
async def batch_init_users(n: int, starting_stock: int, item_price: int):
    n = int(n)
    starting_stock = int(starting_stock)
    item_price = int(item_price)
    kv_pairs: dict[str, bytes] = {f"{i}": msgpack.encode(StockValue(stock=starting_stock, price=item_price))
                                  for i in range(n)}
    try:
        await db.mset(kv_pairs)
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return jsonify({"msg": "Batch init for stock successful"})


@app.get('/find/<item_id>')
async def find_item(item_id: str):
    item_entry: StockValue = await get_item_from_db(item_id)
    return jsonify(
        {
            "stock": item_entry.stock,
            "price": item_entry.price
        }
    )


@app.post('/add/<item_id>/<amount>')
async def add_stock(item_id: str, amount: int):
    item_entry: StockValue = await get_item_from_db(item_id)
    # update stock, serialize and update database
    item_entry.stock += int(amount)
    try:
        await db.set(item_id, msgpack.encode(item_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return Response(f"Item: {item_id} stock updated to: {item_entry.stock}", status=200)


@app.post('/subtract/<item_id>/<amount>')
async def remove_stock(item_id: str, amount: int):
    item_entry: StockValue = await get_item_from_db(item_id)
    # update stock, serialize and update database
    item_entry.stock -= int(amount)
    app.logger.debug(f"Item: {item_id} stock updated to: {item_entry.stock}")
    if item_entry.stock < 0:
        abort(400, f"Item: {item_id} stock cannot get reduced below zero!")
    try:
        await db.set(item_id, msgpack.encode(item_entry))
    except redis.exceptions.RedisError:
        return abort(400, DB_ERROR_STR)
    return Response(f"Item: {item_id} stock updated to: {item_entry.stock}", status=200)


async def handle_events(event):
    event_type = event.get("type")
    if event_type == "order":
        logging.info(f"Received order event: {event}")
    if event_type == "app-event":
        logging.info(f"Received app event: {event}")

@app.before_serving
async def startup():
    app.logger.info("Starting Stock Service")
    await KafkaConsumerSingleton.get_instance(
        TOPICS,
        KAFKA_BOOTSTRAP_SERVERS,
        "stock-group",
        handle_events
    )

@app.after_serving
async def shutdown():
    app.logger.info("Stopping Stock Service")
    await KafkaConsumerSingleton.close()
    await close_db_connection()

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)
    app.logger.setLevel(logging.INFO)
    hypercorn_logger = logging.getLogger('hypercorn.error')
    app.logger.handlers = hypercorn_logger.handlers
    app.logger.setLevel(hypercorn_logger.level)
else:
    hypercorn_logger = logging.getLogger('hypercorn.error')
    app.logger.handlers = hypercorn_logger.handlers
    app.logger.setLevel(hypercorn_logger.level)

  // File: stock/requirements.txt
Quart==0.20.0
redis==5.0.3
hypercorn==0.15.0 
msgspec==0.18.6
requests==2.31.0
aiokafka==0.9.0  
opentelemetry-distro
opentelemetry-exporter-otlp-proto-grpc
// Directory: test
  // Directory: test/__pycache__
  // File: test/test_microservices.py
import unittest

import utils as tu


class TestMicroservices(unittest.TestCase):

    def test_stock(self):
        # Test /stock/item/create/<price>
        item: dict = tu.create_item(5)
        self.assertIn('item_id', item)

        item_id: str = item['item_id']

        # Test /stock/find/<item_id>
        item: dict = tu.find_item(item_id)
        self.assertEqual(item['price'], 5)
        self.assertEqual(item['stock'], 0)

        # Test /stock/add/<item_id>/<number>
        add_stock_response = tu.add_stock(item_id, 50)
        self.assertTrue(200 <= int(add_stock_response) < 300)

        stock_after_add: int = tu.find_item(item_id)['stock']
        self.assertEqual(stock_after_add, 50)

        # Test /stock/subtract/<item_id>/<number>
        over_subtract_stock_response = tu.subtract_stock(item_id, 200)
        self.assertTrue(tu.status_code_is_failure(int(over_subtract_stock_response)))

        subtract_stock_response = tu.subtract_stock(item_id, 15)
        self.assertTrue(tu.status_code_is_success(int(subtract_stock_response)))

        stock_after_subtract: int = tu.find_item(item_id)['stock']
        self.assertEqual(stock_after_subtract, 35)

    def test_payment(self):
        # Test /payment/pay/<user_id>/<order_id>
        user: dict = tu.create_user()
        self.assertIn('user_id', user)

        user_id: str = user['user_id']

        # Test /users/credit/add/<user_id>/<amount>
        add_credit_response = tu.add_credit_to_user(user_id, 15)
        self.assertTrue(tu.status_code_is_success(add_credit_response))

        # add item to the stock service
        item: dict = tu.create_item(5)
        self.assertIn('item_id', item)

        item_id: str = item['item_id']

        add_stock_response = tu.add_stock(item_id, 50)
        self.assertTrue(tu.status_code_is_success(add_stock_response))

        # create order in the order service and add item to the order
        order: dict = tu.create_order(user_id)
        self.assertIn('order_id', order)

        order_id: str = order['order_id']

        add_item_response = tu.add_item_to_order(order_id, item_id, 1)
        self.assertTrue(tu.status_code_is_success(add_item_response))

        add_item_response = tu.add_item_to_order(order_id, item_id, 1)
        self.assertTrue(tu.status_code_is_success(add_item_response))
        add_item_response = tu.add_item_to_order(order_id, item_id, 1)
        self.assertTrue(tu.status_code_is_success(add_item_response))

        payment_response = tu.payment_pay(user_id, 10)
        self.assertTrue(tu.status_code_is_success(payment_response))

        credit_after_payment: int = tu.find_user(user_id)['credit']
        self.assertEqual(credit_after_payment, 5)

    def test_order(self):
        # Test /payment/pay/<user_id>/<order_id>
        user: dict = tu.create_user()
        self.assertIn('user_id', user)

        user_id: str = user['user_id']

        # create order in the order service and add item to the order
        order: dict = tu.create_order(user_id)
        self.assertIn('order_id', order)

        order_id: str = order['order_id']

        # add item to the stock service
        item1: dict = tu.create_item(5)
        self.assertIn('item_id', item1)
        item_id1: str = item1['item_id']
        add_stock_response = tu.add_stock(item_id1, 15)
        self.assertTrue(tu.status_code_is_success(add_stock_response))

        # add item to the stock service
        item2: dict = tu.create_item(5)
        self.assertIn('item_id', item2)
        item_id2: str = item2['item_id']
        add_stock_response = tu.add_stock(item_id2, 1)
        self.assertTrue(tu.status_code_is_success(add_stock_response))

        add_item_response = tu.add_item_to_order(order_id, item_id1, 1)
        self.assertTrue(tu.status_code_is_success(add_item_response))
        add_item_response = tu.add_item_to_order(order_id, item_id2, 1)
        self.assertTrue(tu.status_code_is_success(add_item_response))
        subtract_stock_response = tu.subtract_stock(item_id2, 1)
        self.assertTrue(tu.status_code_is_success(subtract_stock_response))

        checkout_response = tu.checkout_order(order_id).status_code
        self.assertTrue(tu.status_code_is_failure(checkout_response))

        stock_after_subtract: int = tu.find_item(item_id1)['stock']
        self.assertEqual(stock_after_subtract, 15)

        add_stock_response = tu.add_stock(item_id2, 15)
        self.assertTrue(tu.status_code_is_success(int(add_stock_response)))

        credit_after_payment: int = tu.find_user(user_id)['credit']
        self.assertEqual(credit_after_payment, 0)

        checkout_response = tu.checkout_order(order_id).status_code
        self.assertTrue(tu.status_code_is_failure(checkout_response))

        add_credit_response = tu.add_credit_to_user(user_id, 15)
        self.assertTrue(tu.status_code_is_success(int(add_credit_response)))

        credit: int = tu.find_user(user_id)['credit']
        self.assertEqual(credit, 15)

        stock: int = tu.find_item(item_id1)['stock']
        self.assertEqual(stock, 15)

        checkout_response = tu.checkout_order(order_id)
        print(checkout_response.text)
        self.assertTrue(tu.status_code_is_success(checkout_response.status_code))

        stock_after_subtract: int = tu.find_item(item_id1)['stock']
        self.assertEqual(stock_after_subtract, 14)

        credit: int = tu.find_user(user_id)['credit']
        self.assertEqual(credit, 5)


if __name__ == '__main__':
    unittest.main()

  // File: test/utils.py
import requests

ORDER_URL = STOCK_URL = PAYMENT_URL = "http://127.0.0.1:8000"


########################################################################################################################
#   STOCK MICROSERVICE FUNCTIONS
########################################################################################################################
def create_item(price: int) -> dict:
    return requests.post(f"{STOCK_URL}/stock/item/create/{price}").json()


def find_item(item_id: str) -> dict:
    return requests.get(f"{STOCK_URL}/stock/find/{item_id}").json()


def add_stock(item_id: str, amount: int) -> int:
    return requests.post(f"{STOCK_URL}/stock/add/{item_id}/{amount}").status_code


def subtract_stock(item_id: str, amount: int) -> int:
    return requests.post(f"{STOCK_URL}/stock/subtract/{item_id}/{amount}").status_code


########################################################################################################################
#   PAYMENT MICROSERVICE FUNCTIONS
########################################################################################################################
def payment_pay(user_id: str, amount: int) -> int:
    return requests.post(f"{PAYMENT_URL}/payment/pay/{user_id}/{amount}").status_code


def create_user() -> dict:
    return requests.post(f"{PAYMENT_URL}/payment/create_user").json()


def find_user(user_id: str) -> dict:
    return requests.get(f"{PAYMENT_URL}/payment/find_user/{user_id}").json()


def add_credit_to_user(user_id: str, amount: float) -> int:
    return requests.post(f"{PAYMENT_URL}/payment/add_funds/{user_id}/{amount}").status_code


########################################################################################################################
#   ORDER MICROSERVICE FUNCTIONS
########################################################################################################################
def create_order(user_id: str) -> dict:
    return requests.post(f"{ORDER_URL}/orders/create/{user_id}").json()


def add_item_to_order(order_id: str, item_id: str, quantity: int) -> int:
    return requests.post(f"{ORDER_URL}/orders/addItem/{order_id}/{item_id}/{quantity}").status_code


def find_order(order_id: str) -> dict:
    return requests.get(f"{ORDER_URL}/orders/find/{order_id}").json()


def checkout_order(order_id: str) -> requests.Response:
    return requests.post(f"{ORDER_URL}/orders/checkout/{order_id}")


########################################################################################################################
#   STATUS CHECKS
########################################################################################################################
def status_code_is_success(status_code: int) -> bool:
    return 200 <= status_code < 300


def status_code_is_failure(status_code: int) -> bool:
    return 400 <= status_code < 500

